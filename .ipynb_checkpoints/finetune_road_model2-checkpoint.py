#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
finetune_road_model2.py
í•™ìŠµ: PPO + 2â€‘OPT ë¡œì»¬ì„œì¹˜
ê²€ì¦: ë¹ ë¥¸ greedy í‰ê°€ë§Œ (ìƒ˜í”Œ ì œí•œ)
"""

import os
import random
import argparse
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, Subset
from torch.optim import Adam
from torch.optim.lr_scheduler import CosineAnnealingLR
from tqdm import tqdm
import numpy as np

# ì‚¬ìš©ì ì •ì˜ ëª¨ë“ˆ
from nets.attention_model import AttentionModel
from define_tsp_road import TSPRoad, RoadDataset

# -------------------------
# CLI ì¸ì
# -------------------------
parser = argparse.ArgumentParser(description="PPO + 2-OPT ë¡œì»¬ì„œì¹˜ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸")
parser.add_argument('--resume', action='store_true', help='ì´ì „ì— ì €ì¥ëœ ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì¬ê°œ')
parser.add_argument('--checkpoint', type=str, default='checkpoint.pth', help='ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ')
args = parser.parse_args()

# -------------------------
# í•˜ì´í¼íŒŒë¼ë¯¸í„°
# -------------------------
DEVICE      = "cuda" if torch.cuda.is_available() else "cpu"
PRETRAIN    = "pretrained/tsp_100/epoch-99.pt"
DATA_PKL    = "data/road_TSP_100_fixed.pkl"
OUT_MODEL   = "pretrained/final_ppo_road_v2.pt"
BEST_MODEL  = "pretrained/best_ppo_road_v2.pt"
CHECKPOINT  = args.checkpoint

BATCH_SIZE  = 64
EPOCHS      = 300
LR_INIT     = 3e-5
ETA_MIN     = 1e-7
PPO_EPOCHS  = 3
CLIP_EPS    = 0.2
ENT_COEF    = 0.01
VALUE_COEF  = 0.5
VAL_SPLIT   = 0.1
SEED        = 42

VAL_LIMIT   = 100  # ê²€ì¦ì— ì‚¬ìš©í•  ìµœëŒ€ ìƒ˜í”Œ ìˆ˜

# ì¬í˜„ì„± ê³ ì •
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(SEED)

# -------------------------
# Actor-Critic ì •ì˜
# -------------------------
class ActorCritic(nn.Module):
    def __init__(self, embedding_dim=128, hidden_dim=128, n_encode_layers=3, n_heads=8, tanh_clipping=10.0):
        super().__init__()
        self.actor = AttentionModel(
            embedding_dim=embedding_dim, hidden_dim=hidden_dim,
            n_encode_layers=n_encode_layers, n_heads=n_heads,
            tanh_clipping=tanh_clipping, normalization="batch",
            problem=TSPRoad()
        )
        self.value_head = nn.Sequential(
            nn.Linear(embedding_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 1)
        )

    def forward(self, loc):
        # loc: (batch, nodes, 2)
        _, logp, tour = self.actor(loc, return_pi=True)
        # Critic: actor ì¸ì½”ë” ì¶œë ¥ì˜ í‰ê·  í’€ë§
        # ë‚´ë¶€ êµ¬í˜„ ì˜ì¡´ì ì´ë¯€ë¡œ ê°„ë‹¨íˆ loc í‰ê·  ì‚¬ìš©
        pooled = loc.mean(dim=1)
        value = self.value_head(pooled).squeeze(-1)
        return logp, tour, value

# -------------------------
# ë°ì´í„° ë¡œë“œ ë° ë¶„í• 
# -------------------------
dataset = RoadDataset(DATA_PKL)
n = len(dataset)
indices = list(range(n))
random.shuffle(indices)
split = int(n * VAL_SPLIT)
train_idx, val_idx = indices[split:], indices[:split]
train_loader = DataLoader(Subset(dataset, train_idx), batch_size=BATCH_SIZE, shuffle=True)
val_loader   = DataLoader(Subset(dataset, val_idx),   batch_size=BATCH_SIZE, shuffle=False)

print(f"Dataset: total={n}, train={len(train_idx)}, val={len(val_idx)}")
print(f"Device: {DEVICE}")

# -------------------------
# ëª¨ë¸ / ì˜µí‹°ë§ˆì´ì € / ìŠ¤ì¼€ì¤„ëŸ¬
# -------------------------
env       = TSPRoad()
model     = ActorCritic().to(DEVICE)
optimizer = Adam(model.parameters(), lr=LR_INIT)
scheduler = CosineAnnealingLR(optimizer, T_max=EPOCHS, eta_min=ETA_MIN)

# ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ(ì¬ê°œ)
start_epoch = 1
best_val_cost = float('inf')
if args.resume and os.path.exists(CHECKPOINT):
    ckpt = torch.load(CHECKPOINT, map_location=DEVICE)
    model.load_state_dict(ckpt['model'])
    optimizer.load_state_dict(ckpt['optimizer'])
    scheduler.load_state_dict(ckpt['scheduler'])
    best_val_cost = ckpt.get('best_val_cost', best_val_cost)
    start_epoch = ckpt.get('epoch', 1) + 1
    print(f"ğŸ”„ Resume from epoch {start_epoch}")

# ì‚¬ì „í•™ìŠµ ê°€ì¤‘ì¹˜ ë¡œë“œ
elif start_epoch == 1 and os.path.exists(PRETRAIN):
    pre = torch.load(PRETRAIN, map_location=DEVICE)
    # actor ë¶€ë¶„ë§Œ ë¡œë“œ
    model.actor.load_state_dict(pre.get('model', pre.get('actor')), strict=False)
    print("âœ… Loaded pretrained actor weights")

# -------------------------
# í•™ìŠµ + ê²€ì¦ ë£¨í”„
# -------------------------
for ep in range(start_epoch, EPOCHS+1):
    model.train()
    model.actor.set_decode_type('sampling')
    total_p_loss = total_v_loss = total_e_loss = 0.0

    pbar = tqdm(train_loader, desc=f"Train Ep {ep}/{EPOCHS}", ncols=80)
    for batch in pbar:
        loc  = batch['loc'].to(DEVICE)
        dist = batch['dist'].to(DEVICE)
        # infâ†’í° ê°’ ì¹˜í™˜
        dist = torch.where(torch.isinf(dist), torch.full_like(dist, 1e6), dist)

        # rollout
        logp, tour, value = model(loc)
        cost, _ = env.get_costs({'dist': dist}, tour)
        reward = -cost

        # advantage
        adv = reward - value.detach()
        adv = (adv - adv.mean()) / (adv.std() + 1e-8)

        # PPO ì—…ë°ì´íŠ¸
        old_logp = logp.detach()
        policy_loss = value_loss = entropy = None
        for _ in range(PPO_EPOCHS):
            logp_new, _, value_new = model(loc)
            ratio = torch.exp(logp_new - old_logp)
            s1 = ratio * adv
            s2 = torch.clamp(ratio, 1-CLIP_EPS, 1+CLIP_EPS) * adv
            policy_loss = -torch.min(s1, s2).mean()
            value_loss  = F.mse_loss(value_new, reward)
            entropy     = -logp_new.mean()

            loss = policy_loss + VALUE_COEF * value_loss - ENT_COEF * entropy
            optimizer.zero_grad()
            loss.backward()
            nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()

        total_p_loss += policy_loss.item()
        total_v_loss += value_loss.item()
        total_e_loss += entropy.item()

    scheduler.step()
    print(f"[Ep {ep}] P_loss: {total_p_loss/len(train_loader):.4f}, V_loss: {total_v_loss/len(train_loader):.4f}, Ent: {total_e_loss/len(train_loader):.4f}")

    # â”€â”€ ë¹ ë¥¸ ê²€ì¦ â”€â”€
    model.eval()
    model.actor.set_decode_type('greedy')
    val_cost = 0.0
    cnt = 0

    with torch.no_grad():
        for batch in val_loader:
            if cnt >= VAL_LIMIT: break
            loc, dist = batch['loc'].to(DEVICE), batch['dist'].to(DEVICE)
            _, tour, _ = model(loc, return_pi=True)
            cost, _ = env.get_costs({'dist': dist}, tour)
            val_cost += cost.item()
            cnt += loc.size(0)

    avg_val = val_cost / cnt if cnt>0 else float('nan')
    print(f"[Ep {ep}] ë¹ ë¥¸ê²€ì¦ Val Cost({cnt}ìƒ˜í”Œ): {avg_val:.2f}")

    # ì²´í¬í¬ì¸íŠ¸ ì €ì¥
    torch.save({
        'epoch': ep,
        'model': model.state_dict(),
        'optimizer': optimizer.state_dict(),
        'scheduler': scheduler.state_dict(),
        'best_val_cost': best_val_cost
    }, CHECKPOINT)

    # ìµœê³  ëª¨ë¸ ê°±ì‹ 
    if avg_val < best_val_cost:
        best_val_cost = avg_val
        torch.save(model.state_dict(), BEST_MODEL)
        print(f"ğŸŒŸ New Best Val Cost: {best_val_cost:.2f}")

# ìµœì¢… ëª¨ë¸ ì €ì¥
os.makedirs(os.path.dirname(OUT_MODEL), exist_ok=True)
torch.save(model.state_dict(), OUT_MODEL)
print(f"âœ… Training complete. Best Val Cost: {best_val_cost:.2f}")




