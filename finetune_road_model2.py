import os
import random
import argparse
import pickle
import numpy as np
import torch
import torch.nn.functional as F
from torch.utils.data import DataLoader, Subset
from torch.optim import Adam
from torch.optim.lr_scheduler import CosineAnnealingLR
from tqdm import tqdm

from nets.attention_model import AttentionModel
from define_tsp_road import TSPRoad, RoadDataset

# ----------------------------- #
# ì„¤ì • ë° íŒŒë¼ë¯¸í„°
# ----------------------------- #
parser = argparse.ArgumentParser()
parser.add_argument('--resume', action='store_true')
parser.add_argument('--checkpoint', type=str, default='checkpoint.pth')
parser.add_argument('--data_pkl', type=str, default='data/road_TSP_100_nozero.pkl')
parser.add_argument('--pretrain', type=str, default='pretrained/tsp_100/epoch-99.pt')
parser.add_argument('--out_model', type=str, default='pretrained/final_tsp_road.pt')
parser.add_argument('--best_model', type=str, default='pretrained/best_tsp_road.pt')
parser.add_argument('--batch_size', type=int, default=64)
parser.add_argument('--epochs', type=int, default=300)
parser.add_argument('--lr', type=float, default=1e-5)  # í•™ìŠµë¥  ê°ì†Œ
parser.add_argument('--weight_decay', type=float, default=1e-5)
parser.add_argument('--warmup_epochs', type=int, default=10)
parser.add_argument('--clip_grad', type=float, default=1.0)
args = parser.parse_args()

# ê¸°ë³¸ ì„¤ì •
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
PRETRAIN = args.pretrain
DATA_PKL = args.data_pkl
OUT_MODEL = args.out_model
BEST_MODEL = args.best_model
CHECKPOINT = args.checkpoint
BATCH_SIZE = args.batch_size
EPOCHS = args.epochs
LR_INIT = args.lr
WEIGHT_DECAY = args.weight_decay
WARMUP_EPOCHS = args.warmup_epochs
MAX_GRAD_NORM = args.clip_grad
ETA_MIN = 1e-7
FALLBACK_DIST = 10000.0  # ìµœëŒ€ ê±°ë¦¬ í´ë¦¬í•‘ ê°’ (ë¯¸í„° ë‹¨ìœ„)
VAL_SPLIT = 0.1
SEED = 42

# Seed ê³ ì •
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(SEED)

# ----------------------------- #
# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
# ----------------------------- #
def create_directory(path):
    """ë””ë ‰í† ë¦¬ ìƒì„±"""
    directory = os.path.dirname(path)
    if directory and not os.path.exists(directory):
        os.makedirs(directory, exist_ok=True)

def convert_to_km(meters):
    """ë¯¸í„° ë‹¨ìœ„ ë¹„ìš©ì„ í‚¬ë¡œë¯¸í„°ë¡œ ë³€í™˜"""
    return meters / 1000.0

# ----------------------------- #
# Learning Rate ìŠ¤ì¼€ì¤„ëŸ¬ (Warmup + Cosine)
# ----------------------------- #
class WarmupScheduler:
    def __init__(self, optimizer, warmup_epochs, total_epochs, eta_min=0):
        self.warmup_epochs = warmup_epochs
        self.cosine = CosineAnnealingLR(optimizer, T_max=total_epochs-warmup_epochs, eta_min=eta_min)
        self.optimizer = optimizer
        self.init_lr = optimizer.param_groups[0]['lr']
        self.eta_min = eta_min
        self.current_epoch = 0

    def step(self):
        self.current_epoch += 1
        if self.current_epoch <= self.warmup_epochs:
            factor = self.current_epoch / self.warmup_epochs
            for pg in self.optimizer.param_groups:
                pg['lr'] = self.eta_min + factor * (self.init_lr - self.eta_min)
        else:
            self.cosine.step()

    def get_lr(self):
        return self.optimizer.param_groups[0]['lr']

# ----------------------------- #
# ë©”ì¸ í•¨ìˆ˜
# ----------------------------- #
def main():
    # 1. ë””ë ‰í† ë¦¬ ìƒì„±
    create_directory(OUT_MODEL)
    create_directory(BEST_MODEL)
    create_directory(CHECKPOINT)

    # 2. í™˜ê²½ ë° ëª¨ë¸ ìƒì„±
    env = TSPRoad()
    model = AttentionModel(
        embedding_dim=128,
        hidden_dim=128,
        n_encode_layers=3,
        n_heads=8,
        tanh_clipping=10.0,
        normalization="batch",
        problem=env
    ).to(DEVICE)

    # 3. ë°ì´í„° ë¡œë“œ
    print(f"ë°ì´í„°ì…‹ ë¡œë”© ì¤‘: {DATA_PKL}")
    try:
        full_dataset = RoadDataset(DATA_PKL)
        n_total = len(full_dataset)

        # í•™ìŠµ/ê²€ì¦ ì„¸íŠ¸ ë¶„í• 
        indices = list(range(n_total))
        random.shuffle(indices)
        split = int(n_total * VAL_SPLIT)
        train_idx, val_idx = indices[split:], indices[:split]

                # ì „ì²´ ë°ì´í„°ì…‹ ê±°ë¦¬ í†µê³„ (100ê°œ ìƒ˜í”Œë§Œ)  âš¡ï¸
        all_dists = []
        for idx in range(min(100, n_total)):
            d = full_dataset[idx]['dist']
            all_dists.extend(d.flatten())
        all_arr = np.array(all_dists)
        all_arr = all_arr[(~np.isnan(all_arr)) & (~np.isinf(all_arr)) & (all_arr > 0)]
        print("ê±°ë¦¬ í†µê³„ (100ê°œ ìƒ˜í”Œ, ë¯¸í„°):",
              f"í‰ê· ={all_arr.mean():.2f}, ì¤‘ì•™ê°’={np.median(all_arr):.2f},",
              f"ìµœì†Œ={all_arr.min():.2f}, ìµœëŒ€={all_arr.max():.2f}")

        train_loader = DataLoader(
            Subset(full_dataset, train_idx),
            batch_size=BATCH_SIZE,
            shuffle=True
        )
        val_loader = DataLoader(
            Subset(full_dataset, val_idx),
            batch_size=BATCH_SIZE
        )
        print(f"ë°ì´í„° ë¡œë“œ ì™„ë£Œ: ì´ {n_total}ê°œ (í•™ìŠµ {len(train_idx)}, ê²€ì¦ {len(val_idx)})")
    except Exception as e:
        print(f"ë°ì´í„° ë¡œë“œ ì˜¤ë¥˜: {e}")
        return

    # 4. ì˜µí‹°ë§ˆì´ì € & ìŠ¤ì¼€ì¤„ëŸ¬
    optimizer = Adam(
        model.parameters(),
        lr=LR_INIT,
        weight_decay=WEIGHT_DECAY
    )
    scheduler = WarmupScheduler(
        optimizer,
        warmup_epochs=WARMUP_EPOCHS,
        total_epochs=EPOCHS,
        eta_min=ETA_MIN
    )

    # 5. ëª¨ë¸ ì´ˆê¸°í™”/ë¡œë“œ
    start_epoch = 1
    best_val_cost = float('inf')
    train_history, val_history = [], []

    if args.resume and os.path.exists(CHECKPOINT):
        ckpt = torch.load(CHECKPOINT, map_location=DEVICE)
        model.load_state_dict(ckpt['model'])
        optimizer.load_state_dict(ckpt['optimizer'])
        scheduler.current_epoch = ckpt.get('epoch', 1)
        best_val_cost = ckpt.get('best_val_cost', best_val_cost)
        train_history = ckpt.get('train_history', [])
        val_history = ckpt.get('val_history', [])
        start_epoch = ckpt.get('epoch', 1) + 1
        print(f"ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ: epoch={start_epoch}")
    elif os.path.exists(PRETRAIN):
        pt = torch.load(PRETRAIN, map_location=DEVICE)
        model.load_state_dict(pt.get('model', pt))
        print("ì‚¬ì „í•™ìŠµ ê°€ì¤‘ì¹˜ ë¡œë“œ ì™„ë£Œ")

    # EMA baseline ì„¤ì •
    baseline_ema = None
    ema_alpha = 0.9

    # 6. í•™ìŠµ ë° ê²€ì¦ ë£¨í”„
    for epoch in range(start_epoch, EPOCHS+1):
        # í•™ìŠµ
        model.train()
        total_loss, total_cost = 0.0, 0.0
        for batch in tqdm(train_loader, desc=f"í•™ìŠµ {epoch}/{EPOCHS}", dynamic_ncols=True, mininterval=0.5, ascii=True, smoothing=0):
            loc = batch['loc'].to(DEVICE)
            dist = batch['dist'].to(DEVICE)
            dist = torch.where(
                torch.isnan(dist) | torch.isinf(dist) | (dist<0),
                torch.full_like(dist, FALLBACK_DIST),
                dist
            )
            model.set_decode_type('sampling')
            cost, ll = model({'loc':loc, 'dist':dist})
            # ë‹¨ìœ„ ë³€í™˜ ë° REINFORCE
            B, N, _ = dist.size()
            cost_km = cost / 1000.0
            unit_cost = cost_km / N
            if baseline_ema is None:
                baseline_ema = unit_cost.mean().item()
            baseline_ema = ema_alpha * baseline_ema + (1-ema_alpha) * unit_cost.mean().item()
            adv = (unit_cost - baseline_ema)
            adv = (adv - adv.mean()) / (adv.std(unbiased=False) + 1e-6)
            loss = (adv * ll).mean()
            optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), MAX_GRAD_NORM)
            optimizer.step()
            total_loss += loss.item()
            total_cost += unit_cost.mean().item()

        scheduler.step()
        avg_loss = total_loss / len(train_loader)
        avg_cost = total_cost / len(train_loader)
        train_history.append(avg_cost)
        print(f"[Epoch {epoch}] loss={avg_loss:.4f}, cost={avg_cost*1000:.2f}m")

        # ê²€ì¦
        model.eval()
        val_costs = []
        with torch.no_grad():
            for batch in tqdm(val_loader, desc=f"ê²€ì¦ {epoch}/{EPOCHS}", dynamic_ncols=True, mininterval=0.5, ascii=True, smoothing=0):
                loc = batch['loc'].to(DEVICE)
                dist = batch['dist'].to(DEVICE)
                dist = torch.where(
                    torch.isnan(dist) | torch.isinf(dist) | (dist<0),
                    torch.full_like(dist, FALLBACK_DIST),
                    dist
                )
                model.set_decode_type('greedy')
                cost, _, _ = model({'loc':loc, 'dist':dist}, return_pi=True)
                val_costs.append(cost.mean().item())
        val_arr = np.array(val_costs) / 1000.0
        val_mean_km = val_arr.mean()
        val_history.append(val_mean_km)
        print(f"[Epoch {epoch}] ê²€ì¦ ë¹„ìš©: {val_mean_km*1000:.2f}m ({val_mean_km:.2f}km)")

        # ìµœê³  ëª¨ë¸ ì €ì¥
        if val_mean_km < best_val_cost:
            best_val_cost = val_mean_km
            torch.save({
                'model': model.state_dict(),
                'epoch': epoch,
                'best_val_cost': best_val_cost,
                'train_history': train_history,
                'val_history': val_history
            }, BEST_MODEL)
            print(f"ğŸŒŸ ìµœê³  ëª¨ë¸ ì €ì¥ @epoch{epoch} (Val {best_val_cost*1000:.2f}m)")

        # ì²´í¬í¬ì¸íŠ¸ ì €ì¥
        torch.save({
            'epoch': epoch,
            'model': model.state_dict(),
            'optimizer': optimizer.state_dict(),
            'best_val_cost': best_val_cost,
            'train_history': train_history,
            'val_history': val_history
        }, CHECKPOINT)

    print(f"í•™ìŠµ ì™„ë£Œ. ìµœê³  ê²€ì¦ ë¹„ìš©: {best_val_cost*1000:.2f}m ({best_val_cost:.2f}km)")
    torch.save({'model': model.state_dict(), 'best_val_cost': best_val_cost}, OUT_MODEL)
    print(f"âœ… ìµœì¢… ëª¨ë¸ ì €ì¥: {OUT_MODEL}")

if __name__ == "__main__":
    main()
