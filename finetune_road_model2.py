import os
import random
import argparse
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, Subset
from torch.optim import Adam
from torch.optim.lr_scheduler import CosineAnnealingLR
from tqdm import tqdm
import numpy as np

from nets.attention_model import AttentionModel
from define_tsp_road import TSPRoad, RoadDataset

# ----------------------------- #
# ì„¤ì • ë° íŒŒë¼ë¯¸í„°
# ----------------------------- #
parser = argparse.ArgumentParser()
parser.add_argument('--resume', action='store_true')
parser.add_argument('--checkpoint', type=str, default='checkpoint.pth')
parser.add_argument('--data_pkl', type=str, default='data/road_TSP_100_nozero.pkl')
parser.add_argument('--pretrain', type=str, default='pretrained/tsp_100/epoch-99.pt')
parser.add_argument('--out_model', type=str, default='pretrained/final_tsp_road.pt')
parser.add_argument('--best_model', type=str, default='pretrained/best_tsp_road.pt')
parser.add_argument('--batch_size', type=int, default=64)
parser.add_argument('--epochs', type=int, default=300)
parser.add_argument('--lr', type=float, default=3e-5)
parser.add_argument('--warmup_epochs', type=int, default=10)
parser.add_argument('--clip_grad', type=float, default=1.0)
args = parser.parse_args()

# ê¸°ë³¸ ì„¤ì •
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
PRETRAIN = args.pretrain
DATA_PKL = args.data_pkl
OUT_MODEL = args.out_model
BEST_MODEL = args.best_model
CHECKPOINT = args.checkpoint
BATCH_SIZE = args.batch_size
EPOCHS = args.epochs
LR_INIT = args.lr
ETA_MIN = 1e-7
FALLBACK_DIST = 10000.0  # ìµœëŒ€ ê±°ë¦¬ í´ë¦¬í•‘ ê°’ (ë¯¸í„° ë‹¨ìœ„) - OSRMì€ ë¯¸í„° ë‹¨ìœ„ë¡œ ë°˜í™˜
VAL_SPLIT = 0.1
SEED = 42
MAX_GRAD_NORM = args.clip_grad  # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘ ê°’
WARMUP_EPOCHS = args.warmup_epochs  # ì›œì—… ì—í¬í¬

# Seed ê³ ì •
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(SEED)

# ----------------------------- #
# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
# ----------------------------- #
def create_directory(path):
    """ë””ë ‰í† ë¦¬ ìƒì„±"""
    directory = os.path.dirname(path)
    if directory and not os.path.exists(directory):
        os.makedirs(directory, exist_ok=True)

def fix_bad_values(tensor):
    """ë¬¸ì œê°€ ìˆëŠ” í…ì„œ ê°’ ìˆ˜ì •"""
    return torch.where(
        torch.isnan(tensor) | torch.isinf(tensor), 
        torch.zeros_like(tensor), 
        tensor
    )

# ë‹¨ìœ„ í™˜ì‚° í•¨ìˆ˜
def convert_to_km(meters):
    """ë¯¸í„° ë‹¨ìœ„ ë¹„ìš©ì„ í‚¬ë¡œë¯¸í„°ë¡œ ë³€í™˜"""
    return meters / 1000.0

# ----------------------------- #
# Learning Rate ìŠ¤ì¼€ì¤„ëŸ¬
# ----------------------------- #
class WarmupScheduler:
    """
    ì›œì—… í›„ ì½”ì‚¬ì¸ ìŠ¤ì¼€ì¤„ëŸ¬ ì ìš©
    """
    def __init__(self, optimizer, warmup_epochs, max_epochs, eta_min=0):
        self.warmup_epochs = warmup_epochs
        self.cosine_scheduler = CosineAnnealingLR(optimizer, T_max=max_epochs-warmup_epochs, eta_min=eta_min)
        self.optimizer = optimizer
        self.eta_min = eta_min
        self.initial_lr = optimizer.param_groups[0]['lr']
        self.current_epoch = 0
    
    def step(self):
        self.current_epoch += 1
        if self.current_epoch <= self.warmup_epochs:
            # Linear warmup
            factor = self.current_epoch / self.warmup_epochs
            for param_group in self.optimizer.param_groups:
                param_group['lr'] = self.eta_min + factor * (self.initial_lr - self.eta_min)
        else:
            # Cosine annealing
            self.cosine_scheduler.step()
    
    def get_lr(self):
        return self.optimizer.param_groups[0]['lr']

# ----------------------------- #
# ë©”ì¸ í•¨ìˆ˜
# ----------------------------- #
def main():
    # 1. ë””ë ‰í† ë¦¬ ìƒì„±
    create_directory(OUT_MODEL)
    create_directory(BEST_MODEL)
    create_directory(CHECKPOINT)
    
    # 2. í™˜ê²½ ë° ëª¨ë¸ ìƒì„±
    env = TSPRoad()
    model = AttentionModel(
        embedding_dim=128, 
        hidden_dim=128,
        n_encode_layers=3, 
        n_heads=8,
        tanh_clipping=10.0, 
        normalization="batch",
        problem=env
    ).to(DEVICE)
    
    # 3. ë°ì´í„° ë¡œë“œ
    print(f"ë°ì´í„°ì…‹ ë¡œë”© ì¤‘: {DATA_PKL}")
    try:
        full_dataset = RoadDataset(DATA_PKL)
        n_total = len(full_dataset)
        
        # í•™ìŠµ/ê²€ì¦ ì„¸íŠ¸ ë¶„í• 
        indices = list(range(n_total))
        random.shuffle(indices)
        split = int(n_total * VAL_SPLIT)
        train_idx, val_idx = indices[split:], indices[:split]
        
        # ë°ì´í„° ì „ì²˜ë¦¬ - í‰ê· ê³¼ í‘œì¤€í¸ì°¨ ê³„ì‚°
        dist_values = []
        for idx in range(min(100, n_total)):  # 100ê°œ ìƒ˜í”Œë¡œ í†µê³„ ê³„ì‚°
            sample = full_dataset[idx]
            dist = sample['dist']
            dist_values.extend(dist.flatten().tolist())
        
        dist_values = np.array(dist_values)
        dist_values = dist_values[~np.isnan(dist_values) & ~np.isinf(dist_values) & (dist_values > 0)]
        
        # í†µê³„ ì¶œë ¥
        mean_dist = np.mean(dist_values)
        median_dist = np.median(dist_values)
        min_dist = np.min(dist_values)
        max_dist = np.max(dist_values)
        
        print(f"ê±°ë¦¬ í†µê³„ (ë¯¸í„° ë‹¨ìœ„):")
        print(f"  - í‰ê· : {mean_dist:.2f}m ({convert_to_km(mean_dist):.2f}km)")
        print(f"  - ì¤‘ì•™ê°’: {median_dist:.2f}m ({convert_to_km(median_dist):.2f}km)")
        print(f"  - ìµœì†Œ: {min_dist:.2f}m ({convert_to_km(min_dist):.2f}km)")
        print(f"  - ìµœëŒ€: {max_dist:.2f}m ({convert_to_km(max_dist):.2f}km)")
        
        # ë°ì´í„° ë¡œë”
        train_loader = DataLoader(
            Subset(full_dataset, train_idx), 
            batch_size=BATCH_SIZE, 
            shuffle=True
        )
        val_loader = DataLoader(
            Subset(full_dataset, val_idx), 
            batch_size=BATCH_SIZE
        )
        print(f"ë°ì´í„°ì…‹ ë¡œë“œ ì™„ë£Œ: ì´ {n_total}ê°œ (í•™ìŠµ {len(train_idx)}, ê²€ì¦ {len(val_idx)})")
    except Exception as e:
        print(f"ë°ì´í„° ë¡œë“œ ì˜¤ë¥˜: {e}")
        return
    
    # 4. ì˜µí‹°ë§ˆì´ì € & ìŠ¤ì¼€ì¤„ëŸ¬
    optimizer = Adam(model.parameters(), lr=LR_INIT)
    scheduler = WarmupScheduler(
        optimizer, 
        warmup_epochs=WARMUP_EPOCHS,
        max_epochs=EPOCHS,
        eta_min=ETA_MIN
    )
    
    # 5. ëª¨ë¸ ì´ˆê¸°í™”/ë¡œë“œ
    start_epoch = 1
    best_val_cost = float('inf')
    
    # ì²´í¬í¬ì¸íŠ¸ì—ì„œ ë³µì›
    if args.resume and os.path.exists(CHECKPOINT):
        print(f"ì²´í¬í¬ì¸íŠ¸ ë¡œë”©: {CHECKPOINT}")
        try:
            ckpt = torch.load(CHECKPOINT, map_location=DEVICE)
            model.load_state_dict(ckpt['model'])
            optimizer.load_state_dict(ckpt['optimizer'])
            best_val_cost = ckpt.get('best_val_cost', float('inf'))
            start_epoch = ckpt.get('epoch', 1) + 1
            
            # ìŠ¤ì¼€ì¤„ëŸ¬ ë³µì› (ì›œì—… ìŠ¤ì¼€ì¤„ëŸ¬ëŠ” ìƒíƒœë¥¼ ì €ì¥í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ í˜„ì¬ ì—í¬í¬ë§Œ ì„¤ì •)
            scheduler.current_epoch = start_epoch - 1
            print(f"ì—í¬í¬ {start_epoch}ë¶€í„° í•™ìŠµ ì¬ê°œ")
        except Exception as e:
            print(f"ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì˜¤ë¥˜: {e}")
            print("ì²˜ìŒë¶€í„° í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤")
    
    # ì‚¬ì „ í•™ìŠµ ëª¨ë¸ ë¡œë“œ
    elif os.path.exists(PRETRAIN):
        print(f"ì‚¬ì „í•™ìŠµ ëª¨ë¸ ë¡œë”©: {PRETRAIN}")
        try:
            pt = torch.load(PRETRAIN, map_location=DEVICE)
            if 'model' in pt:
                model.load_state_dict(pt['model'])
            else:
                model.load_state_dict(pt)
            print("âœ… ì‚¬ì „í•™ìŠµ ê°€ì¤‘ì¹˜ ë¡œë“œ ì™„ë£Œ")
        except Exception as e:
            print(f"ì‚¬ì „í•™ìŠµ ëª¨ë¸ ë¡œë“œ ì˜¤ë¥˜: {e}")
            print("âš ï¸ ëœë¤ ì´ˆê¸°í™”ë¡œ ì‹œì‘í•©ë‹ˆë‹¤")
    
    # 6. í•™ìŠµ ì‹œì‘
    print(f"í™˜ê²½: {DEVICE}")
    print(f"ì´ {EPOCHS}ê°œ ì—í¬í¬ í•™ìŠµ ì‹œì‘ (ì›œì—… {WARMUP_EPOCHS}ì—í¬í¬)")
    
    # í•™ìŠµ ì§„í–‰ ê¸°ë¡
    train_costs_history = []
    val_costs_history = []
    
    for epoch in range(start_epoch, EPOCHS + 1):
        # í•™ìŠµ ëª¨ë“œ
        model.train()
        
        # ì—í¬í¬ í†µê³„
        epoch_loss = 0
        epoch_reward = 0
        processed_batches = 0
        
        # í•™ìŠµ ë£¨í”„
        train_pbar = tqdm(train_loader, desc=f"í•™ìŠµ {epoch}/{EPOCHS}")
        for batch_idx, batch in enumerate(train_pbar):
            try:
                # ë°°ì¹˜ ë°ì´í„° ì¤€ë¹„
                loc, dist = batch['loc'].to(DEVICE), batch['dist'].to(DEVICE)
                
                # ë¹„ì •ìƒ ê°’ ì œê±°
                dist = torch.where(
                    torch.isnan(dist) | torch.isinf(dist) | (dist < 0),
                    torch.tensor(FALLBACK_DIST, device=DEVICE),
                    dist
                )
                
                # ëª¨ë¸ ì…ë ¥ ì¤€ë¹„
                batch_input = {'loc': loc, 'dist': dist}
                
                # ì¤‘ìš”: AttentionModelì˜ ë””ì½”ë”© íƒ€ì… ëª…ì‹œì  ì„¤ì •
                model.set_decode_type('sampling')
                
                # cost, llì„ ì§ì ‘ ì–»ê¸° ìœ„í•´ ëª¨ë¸ í˜¸ì¶œ (return_pi=Falseë¡œ ì„¤ì •)
                cost, ll = model(batch_input)
                
                # ì†ì‹¤ ê³„ì‚° (log_likelihoodë¥¼ ìµœëŒ€í™”í•˜ëŠ” ê²ƒì´ ëª©í‘œ)
                loss = -ll.mean()
                
                # ì—­ì „íŒŒ ë° ìµœì í™”
                optimizer.zero_grad()
                loss.backward()
                torch.nn.utils.clip_grad_norm_(model.parameters(), MAX_GRAD_NORM)
                optimizer.step()
                
                # í†µê³„ ì—…ë°ì´íŠ¸
                epoch_loss += loss.item()
                # costsëŠ” ì´ë¯¸ ë¯¸í„° ë‹¨ìœ„
                cost_m = cost.mean().item()
                epoch_reward += -cost_m  # ë©”íŠ¸ë¦­ ë‹¨ìœ„ë¡œ ì €ì¥
                processed_batches += 1
                
                # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
                train_pbar.set_postfix(
                    loss=f"{loss.item():.4f}",
                    avg_loss=f"{epoch_loss/processed_batches:.4f}",
                    reward=f"{-cost_m:.2f}m ({convert_to_km(-cost_m):.2f}km)"  # í‚¬ë¡œë¯¸í„° ë° ë¯¸í„° ë‹¨ìœ„ë¡œ í‘œì‹œ
                )
            except Exception as e:
                print(f"\në°°ì¹˜ {batch_idx} í•™ìŠµ ì¤‘ ì˜¤ë¥˜: {e}")
                import traceback
                print(traceback.format_exc())
                continue
        
        # í•™ìŠµë¥  ì—…ë°ì´íŠ¸
        scheduler.step()
        
        # ì—í¬í¬ í†µê³„ ì¶œë ¥
        if processed_batches > 0:
            avg_loss = epoch_loss / processed_batches
            avg_reward = epoch_reward / processed_batches
            train_costs_history.append(-avg_reward)  # ë¹„ìš©ì€ ìŒìˆ˜ ë³´ìƒ
            
            print(f"[ì—í¬í¬ {epoch:3d}] í•™ìŠµ ì†ì‹¤: {avg_loss:.4f} | " 
                  f"ë¹„ìš©: {-avg_reward:.2f}m ({convert_to_km(-avg_reward):.2f}km) | "
                  f"í•™ìŠµë¥ : {scheduler.get_lr():.2e}")
        else:
            print(f"[ì—í¬í¬ {epoch:3d}] ê²½ê³ : ìœ íš¨í•œ ë°°ì¹˜ê°€ ì—†ìŠµë‹ˆë‹¤")
        
        # ê²€ì¦
        model.eval()
        model.set_decode_type('greedy')  # ê²€ì¦ì—ëŠ” greedy ì‚¬ìš©
        
        val_costs = []
        val_pbar = tqdm(val_loader, desc=f"ê²€ì¦ {epoch}/{EPOCHS}")
        
        with torch.no_grad():
            for batch in val_pbar:
                try:
                    # ë°°ì¹˜ ë°ì´í„° ì¤€ë¹„
                    loc, dist = batch['loc'].to(DEVICE), batch['dist'].to(DEVICE)
                    
                    # ë¹„ì •ìƒ ê°’ ì œê±°
                    dist = torch.where(
                        torch.isnan(dist) | torch.isinf(dist) | (dist < 0),
                        torch.tensor(FALLBACK_DIST, device=DEVICE),
                        dist
                    )
                    
                    # ëª¨ë¸ ì…ë ¥
                    batch_input = {'loc': loc, 'dist': dist}
                    
                    # ê²€ì¦ì„ ìœ„í•œ ë¹„ìš© ê³„ì‚°
                    # ê²€ì¦ì—ì„œëŠ” return_pi=Trueë¡œ ì„¤ì •í•˜ì—¬ ê²½ë¡œë„ ì–»ìŒ
                    cost, _, pi = model(batch_input, return_pi=True)
                    
                    # ìœ íš¨í•œ ë¹„ìš©ë§Œ ì €ì¥ (ì´ë¯¸ ë¯¸í„° ë‹¨ìœ„)
                    valid_costs = cost[~torch.isnan(cost) & ~torch.isinf(cost)]
                    if len(valid_costs) > 0:
                        val_costs.extend(valid_costs.cpu().numpy())
                
                except Exception as e:
                    print(f"ê²€ì¦ ë°°ì¹˜ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                    import traceback
                    print(traceback.format_exc())
                    continue
        
        # ê²€ì¦ ê²°ê³¼ ê³„ì‚°
        if val_costs:
            val_cost = np.mean(val_costs)
            val_costs_history.append(val_cost)
            print(f"[ì—í¬í¬ {epoch:3d}] ê²€ì¦ ë¹„ìš©: {val_cost:.2f}m ({convert_to_km(val_cost):.2f}km)")
            
            # ìµœê³  ëª¨ë¸ ì €ì¥
            if val_cost < best_val_cost:
                best_val_cost = val_cost
                torch.save({
                    'model': model.state_dict(),
                    'epoch': epoch,
                    'val_cost': val_cost,
                    'val_cost_km': convert_to_km(val_cost)
                }, BEST_MODEL)
                print(f"ğŸŒŸ ìµœê³  ëª¨ë¸ ì €ì¥ @ì—í¬í¬{epoch} (ê²€ì¦ ë¹„ìš©: {val_cost:.2f}m, {convert_to_km(val_cost):.2f}km)")
        else:
            print(f"[ì—í¬í¬ {epoch:3d}] ê²½ê³ : ìœ íš¨í•œ ê²€ì¦ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤")
        
        # ì—í¬í¬ ì§„í–‰ ê·¸ë˜í”„ (10 ì—í¬í¬ë§ˆë‹¤)
        if epoch % 10 == 0 and len(train_costs_history) > 0:
            print(f"\n===== í•™ìŠµ ì§„í–‰ ìƒí™© (ì—í¬í¬ {epoch}) =====")
            print(f"ìµœê·¼ 10 ì—í¬í¬ í‰ê·  í•™ìŠµ ë¹„ìš©: {np.mean(train_costs_history[-10:]):.2f}m ({convert_to_km(np.mean(train_costs_history[-10:])):.2f}km)")
            if len(val_costs_history) > 0:
                print(f"ìµœê·¼ 10 ì—í¬í¬ í‰ê·  ê²€ì¦ ë¹„ìš©: {np.mean(val_costs_history[-10:]):.2f}m ({convert_to_km(np.mean(val_costs_history[-10:])):.2f}km)")
                print(f"í˜„ì¬ ìµœê³  ê²€ì¦ ë¹„ìš©: {best_val_cost:.2f}m ({convert_to_km(best_val_cost):.2f}km)")
            print("=" * 40)
        
        # ì²´í¬í¬ì¸íŠ¸ ì €ì¥
        torch.save({
            'epoch': epoch,
            'model': model.state_dict(),
            'optimizer': optimizer.state_dict(),
            'best_val_cost': best_val_cost,
            'best_val_cost_km': convert_to_km(best_val_cost),
            'train_costs_history': train_costs_history,
            'val_costs_history': val_costs_history
        }, CHECKPOINT)
        
        # ì£¼ê¸°ì  ì²´í¬í¬ì¸íŠ¸ ì €ì¥
        if epoch % 10 == 0:
            cp_dir = os.path.dirname(CHECKPOINT)
            if cp_dir:
                cp_path = os.path.join(cp_dir, f"checkpoint_ep{epoch}.pth")
            else:
                cp_path = f"checkpoint_ep{epoch}.pth"
                
            torch.save({
                'epoch': epoch,
                'model': model.state_dict(),
                'optimizer': optimizer.state_dict(),
                'best_val_cost': best_val_cost,
                'best_val_cost_km': convert_to_km(best_val_cost),
                'train_costs_history': train_costs_history,
                'val_costs_history': val_costs_history
            }, cp_path)
            print(f"ğŸ“ ì—í¬í¬ {epoch} ì²´í¬í¬ì¸íŠ¸ ì €ì¥")
    
    # í•™ìŠµ ì™„ë£Œ
    print("\n" + "="*50)
    print(f"í•™ìŠµ ì™„ë£Œ. ìµœê³  ê²€ì¦ ë¹„ìš©: {best_val_cost:.2f}m ({convert_to_km(best_val_cost):.2f}km)")
    
    # ìµœì¢… ëª¨ë¸ ì €ì¥
    torch.save({
        'model': model.state_dict(),
        'best_val_cost': best_val_cost,
        'best_val_cost_km': convert_to_km(best_val_cost),
        'final_epoch': EPOCHS,
        'train_costs_history': train_costs_history,
        'val_costs_history': val_costs_history
    }, OUT_MODEL)
    
    print(f"âœ… ìµœì¢… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {OUT_MODEL}")
    print(f"âœ… ìµœê³  ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {BEST_MODEL}")

if __name__ == "__main__":
    main()